{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.41.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.13.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (0.23.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (70.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file generated: post_fact_matches.csv\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load CSV data\n",
    "file_path_facts = \"fact_checks.csv\"\n",
    "file_path_posts = \"posts.csv\"\n",
    "\n",
    "data_facts = pd.read_csv(file_path_facts)\n",
    "data_posts = pd.read_csv(file_path_posts, sep=\",\")\n",
    "\n",
    "# Fill missing data\n",
    "data_posts['ocr'] = data_posts['ocr'].fillna(\"[]\")\n",
    "data_facts['claim'] = data_facts['claim'].fillna(\"[]\")\n",
    "data_posts['text'] = data_posts['text'].fillna(\"\")\n",
    "\n",
    "# Safely apply ast.literal_eval to convert string lists to actual lists\n",
    "def safe_literal_eval(value):\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []  \n",
    "\n",
    "data_posts['ocr'] = data_posts['ocr'].apply(safe_literal_eval)\n",
    "data_facts['claim'] = data_facts['claim'].apply(safe_literal_eval)\n",
    "\n",
    "# Extract OCR-based posts as (post_id, ocr_text)\n",
    "posts_list = [(post_id, item[0]) for post_id, ocr in zip(data_posts['post_id'], data_posts['ocr']) for item in ocr]\n",
    "post_ids_ocr, post_texts_ocr = zip(*posts_list) if posts_list else ([], [])\n",
    "\n",
    "# Extract fact-check data as (fact_check_id, claim_text)\n",
    "facts_list = [(row['fact_check_id'], row['claim'][0]) for _, row in data_facts.iterrows() if row['claim']]\n",
    "fact_check_ids, fact_texts = zip(*facts_list) if facts_list else ([], [])\n",
    "\n",
    "# Create mapping from fact_check_id to title\n",
    "fact_id_to_title = pd.Series(data_facts.title.values, index=data_facts.fact_check_id).to_dict()\n",
    "\n",
    "# Initialize list to store CSV rows\n",
    "rows = []\n",
    "\n",
    "# Process posts if there are fact checks\n",
    "if fact_texts:\n",
    "    # Initialize sentence transformer model\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Encode all fact claims\n",
    "    fact_embeddings = model.encode(fact_texts)\n",
    "    \n",
    "    # Process OCR-based posts\n",
    "    if post_texts_ocr:\n",
    "        # Encode OCR texts in batch\n",
    "        post_embeddings_ocr = model.encode(post_texts_ocr)\n",
    "        # Calculate all similarities at once\n",
    "        ocr_similarities = cosine_similarity(post_embeddings_ocr, fact_embeddings)\n",
    "        \n",
    "        for i, (post_id, ocr_text) in enumerate(zip(post_ids_ocr, post_texts_ocr)):\n",
    "            similarities = ocr_similarities[i]\n",
    "            top_k_indices = similarities.argsort()[-10:][::-1]\n",
    "            for idx in top_k_indices:\n",
    "                fact_id = fact_check_ids[idx]\n",
    "                rows.append({\n",
    "                    'post_id': post_id,\n",
    "                    'ocr_text': ocr_text,\n",
    "                    'fact_check_id': fact_id,\n",
    "                    'title': fact_id_to_title.get(fact_id, \"\"),\n",
    "                    'similarity': similarities[idx]\n",
    "                })\n",
    "    \n",
    "    # Process posts without OCR (use 'text' column)\n",
    "    processed_post_ids = set(post_ids_ocr)\n",
    "    all_post_ids = set(data_posts['post_id'])\n",
    "    unprocessed_post_ids = list(all_post_ids - processed_post_ids)\n",
    "    \n",
    "    if unprocessed_post_ids:\n",
    "        unprocessed_data = data_posts[data_posts['post_id'].isin(unprocessed_post_ids)]\n",
    "        post_texts = unprocessed_data['text'].tolist()\n",
    "        \n",
    "        # Encode text posts in batch\n",
    "        post_embeddings_text = model.encode(post_texts)\n",
    "        text_similarities = cosine_similarity(post_embeddings_text, fact_embeddings)\n",
    "        \n",
    "        for i, (post_id, text) in enumerate(zip(unprocessed_data['post_id'], post_texts)):\n",
    "            similarities = text_similarities[i]\n",
    "            top_k_indices = similarities.argsort()[-10:][::-1]\n",
    "            for idx in top_k_indices:\n",
    "                fact_id = fact_check_ids[idx]\n",
    "                rows.append({\n",
    "                    'post_id': post_id,\n",
    "                    'ocr_text': text,\n",
    "                    'fact_check_id': fact_id,\n",
    "                    'title': fact_id_to_title.get(fact_id, \"\"),\n",
    "                    'similarity': similarities[idx]\n",
    "                })\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "if rows:\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.sort_values(by=['post_id', 'similarity'], ascending=[True, False])\n",
    "    df.to_csv(\"post_fact_matches.csv\", index=False)\n",
    "    print(\"CSV file generated: post_fact_matches.csv\")\n",
    "else:\n",
    "    print(\"No matches found. CSV file not created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JSON written to monolingual_predictions.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# File paths\n",
    "json_file = \"monolingual_predictions.json\"\n",
    "csv_file = \"post_fact_matches.csv\"\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as jf:\n",
    "    monolingual_data = json.load(jf)\n",
    "\n",
    "# Load the CSV data\n",
    "csv_data = []\n",
    "with open(csv_file, \"r\", encoding=\"utf-8\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    for row in reader:\n",
    "        csv_data.append(row)\n",
    "\n",
    "# Populate the JSON data with fact_check_id lists (convert to integers)\n",
    "for post_id in monolingual_data.keys():\n",
    "    fact_check_ids = [\n",
    "        int(row[\"fact_check_id\"]) for row in csv_data if row[\"post_id\"] == post_id\n",
    "    ]\n",
    "    monolingual_data[post_id] = fact_check_ids\n",
    "\n",
    "# Write the updated JSON data to a new file in one line\n",
    "with open(json_file, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    json.dump(monolingual_data, output_file, ensure_ascii=False, separators=(\", \", \": \"))\n",
    "\n",
    "print(f\"Updated JSON written to {json_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created predictions.zip containing: monolingual_predictions.json\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# File paths to include in the zip\n",
    "files_to_zip = [\"monolingual_predictions.json\"]  \n",
    "output_zipfile = \"predictions.zip\"\n",
    "\n",
    "# Create a zip file and add the specified files\n",
    "with zipfile.ZipFile(output_zipfile, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for file in files_to_zip:\n",
    "        zipf.write(file, arcname=file) \n",
    "\n",
    "print(f\"Created {output_zipfile} containing: {', '.join(files_to_zip)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
