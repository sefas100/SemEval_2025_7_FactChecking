{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking path: .\\posts.csv\n",
      "Checking path: .\\fact_checks.csv\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "our_dataset_path = '.'\n",
    "\n",
    "posts_path = os.path.join(our_dataset_path, 'posts.csv')\n",
    "fact_checks_path = os.path.join(our_dataset_path, 'fact_checks.csv')\n",
    "\n",
    "for path in [posts_path, fact_checks_path]:\n",
    "    print(f\"Checking path: {path}\")\n",
    "    assert os.path.isfile(path), f\"File not found: {path}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This is purely to sort the CVs numbers and not otherwise relevant to the main code - please disable for the code to work as intended\n",
    "# import numpy as np\n",
    "\n",
    "# df = pd.read_csv('pairs.csv', header=None)\n",
    "# df.columns = ['post_id', 'fact_check_id']\n",
    "# df.sort_values('post_id', ascending=False, inplace=True, key=lambda s: s.groupby(s).transform(\"size\"))\n",
    "# print (df.sort_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_col = lambda s: ast.literal_eval(s.replace('\\n', '\\\\n')) if s else s\n",
    "\n",
    "# df_fact_checks = pd.read_csv(fact_checks_path).fillna('').set_index('fact_check_id')\n",
    "# for col in ['claim', 'instances', 'title']:\n",
    "#     df_fact_checks[col] = df_fact_checks[col].apply(parse_col)\n",
    "\n",
    "\n",
    "# df_posts = pd.read_csv(posts_path).fillna('').set_index('post_id')\n",
    "# for col in ['instances', 'ocr', 'verdicts', 'text']:\n",
    "#     df_posts[col] = df_posts[col].apply(parse_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All possible verdicts \n",
    "#  import csv\n",
    "\n",
    "# # Der Pfad zur CSV-Datei\n",
    "# csv_datei = posts_path\n",
    "\n",
    "# # Öffnen der CSV-Datei und auslesen der 'verdicts' Spalte\n",
    "# with open(csv_datei, newline='', encoding='utf-8') as csvfile:\n",
    "#     # CSV-Reader erstellen\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "    \n",
    "#     # Set für einzigartige verdicts-Werte\n",
    "#     verdicts_set = set()\n",
    "\n",
    "#     # Iteriere durch jede Zeile in der CSV-Datei\n",
    "#     for row in reader:\n",
    "#         # Füge den Wert der 'verdicts'-Spalte zum Set hinzu\n",
    "#         verdicts_set.add(row['verdicts'])\n",
    "\n",
    "#     # Ausgabe der einzigartigen 'verdicts'-Werte\n",
    "#     print(\"Mögliche 'verdicts'-Werte:\")\n",
    "#     for verdict in sorted(verdicts_set):\n",
    "#         print(verdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print the output\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the CSV file\n",
    "# df = pd.read_csv(\"merged_output.csv\")\n",
    "\n",
    "# # Set Pandas options to display full content without truncation\n",
    "# pd.set_option(\"display.max_colwidth\", None)  # No truncation for column width\n",
    "# pd.set_option(\"display.max_rows\", None)  # Show all rows (use with caution for large datasets)\n",
    "# pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "# pd.set_option(\"display.width\", None)  # Automatically adjust width\n",
    "\n",
    "# # Print the first row of every column\n",
    "# for p in df.iloc[0].values:\n",
    "#     print(p)  # Access the first row (index 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import numpy as np\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import pandas as pd\n",
    "\n",
    "# # Hyperparameters\n",
    "# batch_size = 64\n",
    "# num_epochs = 10\n",
    "# learning_rate = 0.001\n",
    "# hidden_size = 512\n",
    "# max_length = 50\n",
    "\n",
    "# class FactCheckDataset(Dataset):\n",
    "#     def __init__(self, ocr_texts, titles, labels, max_length):\n",
    "#         self.ocr_texts = ocr_texts\n",
    "#         self.titles = titles\n",
    "#         self.labels = labels\n",
    "#         self.max_length = max_length\n",
    "#         self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "#         # Fit TF-IDF on concatenated text for embeddings\n",
    "#         self.vectorizer.fit(ocr_texts + titles)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.ocr_texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         ocr_text = self.ocr_texts[idx]\n",
    "#         title = self.titles[idx]\n",
    "#         label = self.labels[idx]\n",
    "        \n",
    "#         # Vectorize texts using TF-IDF\n",
    "#         ocr_vector = self.vectorizer.transform([ocr_text]).toarray()[0]\n",
    "#         title_vector = self.vectorizer.transform([title]).toarray()[0]\n",
    "        \n",
    "#         # Compute Cosine Similarity\n",
    "#         similarity = cosine_similarity(ocr_vector.reshape(1, -1), title_vector.reshape(1, -1))[0][0]\n",
    "\n",
    "#         return {\n",
    "#             'ocr_vector': torch.tensor(ocr_vector, dtype=torch.float32),\n",
    "#             'title_vector': torch.tensor(title_vector, dtype=torch.float32),\n",
    "#             'similarity': torch.tensor([similarity], dtype=torch.float32),\n",
    "#             'label': torch.tensor(label, dtype=torch.long)\n",
    "#         }\n",
    "\n",
    "# class FactCheckModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_classes):\n",
    "#         super(FactCheckModel, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size + 1, hidden_size)  # Include similarity\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "#     def forward(self, ocr_vector, title_vector, similarity):\n",
    "#         combined_vector = torch.cat((ocr_vector, title_vector, similarity), dim=1)\n",
    "#         x = self.fc1(combined_vector)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Load CSV data\n",
    "# file_path = \"merged_output.csv\"\n",
    "# data = pd.read_csv(file_path)\n",
    "\n",
    "# # Extract required fields\n",
    "# ocr_texts = data['ocr'].fillna(\"\").tolist()\n",
    "# titles = data['title'].fillna(\"\").tolist()\n",
    "# labels = data['label'].fillna(0).astype(int).tolist()  # Assuming 'label' column exists\n",
    "\n",
    "# # Initialize Dataset and DataLoader\n",
    "# dataset = FactCheckDataset(ocr_texts, titles, labels, max_length)\n",
    "# train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Model Initialization\n",
    "# input_size = len(dataset.vectorizer.get_feature_names_out())\n",
    "# model = FactCheckModel(input_size=input_size, hidden_size=hidden_size, num_classes=2)\n",
    "\n",
    "# # Optimizer and Loss Function\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Training Loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         ocr_vector = batch['ocr_vector']\n",
    "#         title_vector = batch['title_vector']\n",
    "#         similarity = batch['similarity']\n",
    "#         labels = batch['label']\n",
    "\n",
    "#         # Forward Pass\n",
    "#         outputs = model(ocr_vector, title_vector, similarity)\n",
    "\n",
    "#         # Loss and Optimization\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}\")\n",
    "\n",
    "# # Top-k Relevant Claims Function\n",
    "# def get_top_k_relevant_claims(ocr_text, titles, vectorizer, k=10):\n",
    "#     ocr_vector = vectorizer.transform([ocr_text]).toarray()[0]\n",
    "#     similarities = []\n",
    "#     for title in titles:\n",
    "#         title_vector = vectorizer.transform([title]).toarray()[0]\n",
    "#         similarity = cosine_similarity(ocr_vector.reshape(1, -1), title_vector.reshape(1, -1))[0][0]\n",
    "#         similarities.append(similarity)\n",
    "\n",
    "#     top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "#     return [(titles[idx], similarities[idx]) for idx in top_k_indices]\n",
    "\n",
    "# # Example Query\n",
    "# ocr_example = \"Dreister Impf-Fake von Markus Söder!\"\n",
    "# top_claims = get_top_k_relevant_claims(ocr_example, titles, dataset.vectorizer, k=2)\n",
    "\n",
    "# print(\"Top Claims:\")\n",
    "# for claim, similarity in top_claims:\n",
    "#     print(f\"Claim: {claim}, Similarity: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ocr_title_similarity.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load CSV data\n",
    "file_path_facts = \"fact_checks.csv\"\n",
    "data_facts = pd.read_csv(file_path_facts)\n",
    "file_path_posts = \"posts.csv\"\n",
    "data_posts = pd.read_csv(file_path_posts)\n",
    "\n",
    "# Fill missing data\n",
    "ocr_texts = data_posts['ocr'].fillna(\"\").tolist()\n",
    "titles = data_facts['title'].fillna(\"\").tolist()\n",
    "post_ids = data_posts['post_id'].tolist()  # Extract post IDs\n",
    "fact_check_ids = data_facts['fact_check_id'].tolist()  # Extract fact check IDs\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "ocr_vectors = vectorizer.fit_transform(ocr_texts)  # TF-IDF vectors for OCR texts\n",
    "title_vectors = vectorizer.transform(titles)       # TF-IDF vectors for Titles\n",
    "\n",
    "# Find Top 10 Most Similar Titles for Each OCR Text\n",
    "top_k = 10\n",
    "results_list = []\n",
    "\n",
    "for i, ocr_text in enumerate(ocr_texts):\n",
    "    # Compute cosine similarity between this OCR text and all titles\n",
    "    similarities = cosine_similarity(ocr_vectors[i], title_vectors).flatten()\n",
    "    \n",
    "    # Get indices of top-k similar titles\n",
    "    top_k_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    # Store the top-k titles and their similarity scores\n",
    "    for idx in top_k_indices:\n",
    "        results_list.append({\n",
    "            \"post_id\": post_ids[i],  # Match post ID\n",
    "            \"ocr_text\": ocr_text,\n",
    "            \"fact_check_id\": fact_check_ids[idx],  # Match fact check ID\n",
    "            \"title\": titles[idx],\n",
    "            \"similarity\": similarities[idx]\n",
    "        })\n",
    "\n",
    "# Convert results to a DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.to_csv(\"ocr_title_similarity.csv\", index=False)\n",
    "print(\"Results saved to ocr_title_similarity.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JSON written to monolingual_predictions.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# File paths\n",
    "json_file = \"monolingual_predictions.json\"\n",
    "csv_file = \"ocr_title_similarity.csv\"\n",
    "\n",
    "# Step 1: Load the JSON data\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as jf:\n",
    "    monolingual_data = json.load(jf)\n",
    "\n",
    "# Step 2: Load the CSV data\n",
    "csv_data = []\n",
    "with open(csv_file, \"r\", encoding=\"utf-8\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    for row in reader:\n",
    "        csv_data.append(row)\n",
    "\n",
    "# Step 3: Populate the JSON data with fact_check_id lists (convert to integers)\n",
    "for post_id in monolingual_data.keys():\n",
    "    fact_check_ids = [\n",
    "        int(row[\"fact_check_id\"]) for row in csv_data if row[\"post_id\"] == post_id\n",
    "    ]\n",
    "    monolingual_data[post_id] = fact_check_ids\n",
    "\n",
    "# Step 4: Write the updated JSON data to a new file in one line\n",
    "with open(json_file, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    json.dump(monolingual_data, output_file, ensure_ascii=False, separators=(\", \", \": \"))\n",
    "\n",
    "print(f\"Updated JSON written to {json_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created predictions.zip containing: monolingual_predictions.json\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# File paths to include in the zip\n",
    "files_to_zip = [\"monolingual_predictions.json\"]  # Add more files as needed\n",
    "output_zipfile = \"predictions.zip\"\n",
    "\n",
    "# Step 1: Create a zip file and add the specified files\n",
    "with zipfile.ZipFile(output_zipfile, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for file in files_to_zip:\n",
    "        zipf.write(file, arcname=file)  # arcname keeps the same file name in the zip\n",
    "\n",
    "print(f\"Created {output_zipfile} containing: {', '.join(files_to_zip)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
