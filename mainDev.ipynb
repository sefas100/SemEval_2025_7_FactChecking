{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking path: .\\Data\\posts.csv\n",
      "Checking path: .\\Data\\fact_checks.csv\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "our_dataset_path = '.'\n",
    "\n",
    "posts_path = os.path.join(our_dataset_path, 'Data\\\\posts.csv')\n",
    "fact_checks_path = os.path.join(our_dataset_path, 'Data\\\\fact_checks.csv')\n",
    "\n",
    "for path in [posts_path, fact_checks_path]:\n",
    "    print(f\"Checking path: {path}\")\n",
    "    assert os.path.isfile(path), f\"File not found: {path}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This is purely to sort the CVs numbers and not otherwise relevant to the main code - please disable for the code to work as intended\n",
    "# import numpy as np\n",
    "\n",
    "# df = pd.read_csv('pairs.csv', header=None)\n",
    "# df.columns = ['post_id', 'fact_check_id']\n",
    "# df.sort_values('post_id', ascending=False, inplace=True, key=lambda s: s.groupby(s).transform(\"size\"))\n",
    "# print (df.sort_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_col = lambda s: ast.literal_eval(s.replace('\\n', '\\\\n')) if s else s\n",
    "\n",
    "# df_fact_checks = pd.read_csv(fact_checks_path).fillna('').set_index('fact_check_id')\n",
    "# for col in ['claim', 'instances', 'title']:\n",
    "#     df_fact_checks[col] = df_fact_checks[col].apply(parse_col)\n",
    "\n",
    "\n",
    "# df_posts = pd.read_csv(posts_path).fillna('').set_index('post_id')\n",
    "# for col in ['instances', 'ocr', 'verdicts', 'text']:\n",
    "#     df_posts[col] = df_posts[col].apply(parse_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All possible verdicts \n",
    "#  import csv\n",
    "\n",
    "# # Der Pfad zur CSV-Datei\n",
    "# csv_datei = posts_path\n",
    "\n",
    "# # Öffnen der CSV-Datei und auslesen der 'verdicts' Spalte\n",
    "# with open(csv_datei, newline='', encoding='utf-8') as csvfile:\n",
    "#     # CSV-Reader erstellen\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "    \n",
    "#     # Set für einzigartige verdicts-Werte\n",
    "#     verdicts_set = set()\n",
    "\n",
    "#     # Iteriere durch jede Zeile in der CSV-Datei\n",
    "#     for row in reader:\n",
    "#         # Füge den Wert der 'verdicts'-Spalte zum Set hinzu\n",
    "#         verdicts_set.add(row['verdicts'])\n",
    "\n",
    "#     # Ausgabe der einzigartigen 'verdicts'-Werte\n",
    "#     print(\"Mögliche 'verdicts'-Werte:\")\n",
    "#     for verdict in sorted(verdicts_set):\n",
    "#         print(verdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print the output\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the CSV file\n",
    "# df = pd.read_csv(\"merged_output.csv\")\n",
    "\n",
    "# # Set Pandas options to display full content without truncation\n",
    "# pd.set_option(\"display.max_colwidth\", None)  # No truncation for column width\n",
    "# pd.set_option(\"display.max_rows\", None)  # Show all rows (use with caution for large datasets)\n",
    "# pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "# pd.set_option(\"display.width\", None)  # Automatically adjust width\n",
    "\n",
    "# # Print the first row of every column\n",
    "# for p in df.iloc[0].values:\n",
    "#     print(p)  # Access the first row (index 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV data\n",
    "\n",
    "file_path_facts = \"Data\\\\fact_checks.csv\"\n",
    "data_facts = pd.read_csv(file_path_facts)\n",
    "file_path_posts = \"Data\\\\posts.csv\"\n",
    "data_posts = pd.read_csv(file_path_posts,sep=\",\")\n",
    "\n",
    "# Fill missing data\n",
    "ocr_texts = data_posts['ocr'].fillna(\"\")\n",
    "titles = data_facts['claim'].fillna(\"\")\n",
    "\n",
    "ocr_texts = ocr_texts.apply(lambda x: ast.literal_eval(x))\n",
    "titles = titles.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# Create new lists for posts and facts\n",
    "posts_list = []\n",
    "for sublist in ocr_texts:\n",
    "    for item in sublist:\n",
    "        posts_list.append(item[0])\n",
    "\n",
    "facts_list = []\n",
    "for item in titles:\n",
    "    facts_list.append(item[0])\n",
    "\n",
    "post_ids = data_posts['post_id'].tolist()  # Extract post IDs\n",
    "fact_check_ids = data_facts['fact_check_id'].tolist()  # Extract fact check IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ocr_title_similarity.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "ocr_vectors = vectorizer.fit_transform(facts_list)  # TF-IDF vectors for OCR texts\n",
    "title_vectors = vectorizer.transform(posts_list)       # TF-IDF vectors for Titles\n",
    "\n",
    "# Find Top 10 Most Similar Titles for Each OCR Text\n",
    "top_k = 10\n",
    "results_list = []\n",
    "\n",
    "for i, ocr_text in enumerate(ocr_texts):\n",
    "    # Compute cosine similarity between this OCR text and all titles\n",
    "    similarities = cosine_similarity(ocr_vectors[i], title_vectors).flatten()\n",
    "    \n",
    "    # Get indices of top-k similar titles\n",
    "    top_k_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    # Store the top-k titles and their similarity scores\n",
    "    for idx in top_k_indices:\n",
    "        results_list.append({\n",
    "            \"post_id\": post_ids[i],  # Match post ID\n",
    "            \"ocr_text\": ocr_text,\n",
    "            \"fact_check_id\": fact_check_ids[idx],  # Match fact check ID\n",
    "            \"title\": titles[idx],\n",
    "            \"similarity\": similarities[idx]\n",
    "        })\n",
    "\n",
    "# Convert results to a DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.to_csv(\"ocr_title_similarity.csv\", index=False)\n",
    "print(\"Results saved to ocr_title_similarity.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JSON written to Data\\monolingual_predictions.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# File paths\n",
    "json_file = \"Data\\\\monolingual_predictions.json\"\n",
    "csv_file = \"ocr_title_similarity.csv\"\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as jf:\n",
    "    monolingual_data = json.load(jf)\n",
    "\n",
    "# Load the CSV data\n",
    "csv_data = []\n",
    "with open(csv_file, \"r\", encoding=\"utf-8\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    for row in reader:\n",
    "        csv_data.append(row)\n",
    "\n",
    "# Populate the JSON data with fact_check_id lists (convert to integers)\n",
    "for post_id in monolingual_data.keys():\n",
    "    fact_check_ids = [\n",
    "        int(row[\"fact_check_id\"]) for row in csv_data if row[\"post_id\"] == post_id\n",
    "    ]\n",
    "    monolingual_data[post_id] = fact_check_ids\n",
    "\n",
    "# Write the updated JSON data to a new file in one line\n",
    "with open(json_file, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    json.dump(monolingual_data, output_file, ensure_ascii=False, separators=(\", \", \": \"))\n",
    "\n",
    "print(f\"Updated JSON written to {json_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created predictions.zip containing: monolingual_predictions.json\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# File paths to include in the zip\n",
    "files_to_zip = [\"monolingual_predictions.json\"]  # Add more files as needed\n",
    "output_zipfile = \"predictions.zip\"\n",
    "\n",
    "# Create a zip file and add the specified files\n",
    "with zipfile.ZipFile(output_zipfile, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for file in files_to_zip:\n",
    "        zipf.write(file, arcname=file)  # arcname keeps the same file name in the zip\n",
    "\n",
    "print(f\"Created {output_zipfile} containing: {', '.join(files_to_zip)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
