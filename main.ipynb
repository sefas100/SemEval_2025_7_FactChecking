{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking path: .\\Data\\posts.csv\n",
      "Checking path: .\\Data\\fact_checks.csv\n",
      "Checking path: .\\Data\\pairs.csv\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "our_dataset_path = '.'\n",
    "\n",
    "posts_path = os.path.join(our_dataset_path, 'Data\\\\posts.csv')\n",
    "fact_checks_path = os.path.join(our_dataset_path, 'Data\\\\fact_checks.csv')\n",
    "fact_check_post_mapping_path = os.path.join(our_dataset_path, 'Data\\\\pairs.csv')\n",
    "\n",
    "for path in [posts_path, fact_checks_path, fact_check_post_mapping_path]:\n",
    "    print(f\"Checking path: {path}\")\n",
    "    assert os.path.isfile(path), f\"File not found: {path}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.sort_values of       post_id fact_check_id\n",
      "343      5767         19000\n",
      "352      5767        143651\n",
      "344      5767         19801\n",
      "345      5767         98205\n",
      "346      5767        140676\n",
      "...       ...           ...\n",
      "10955    2321         62043\n",
      "10954     699         62043\n",
      "10953   24032         62038\n",
      "10952   10828         62028\n",
      "25743   25831        202891\n",
      "\n",
      "[25744 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "#This is purely to sort the CVs numbers and not otherwise relevant to the main code - please disable for the code to work as intended\n",
    "#import numpy as np\n",
    "\n",
    "#df = pd.read_csv('Data\\\\pairs.csv', header=None)\n",
    "#df.columns = ['post_id', 'fact_check_id']\n",
    "#df.sort_values('post_id', ascending=False, inplace=True, key=lambda s: s.groupby(s).transform(\"size\"))\n",
    "#print (df.sort_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_col = lambda s: ast.literal_eval(s.replace('\\n', '\\\\n')) if s else s\n",
    "\n",
    "df_fact_checks = pd.read_csv(fact_checks_path).fillna('').set_index('fact_check_id')\n",
    "for col in ['claim', 'instances', 'title']:\n",
    "    df_fact_checks[col] = df_fact_checks[col].apply(parse_col)\n",
    "\n",
    "\n",
    "df_posts = pd.read_csv(posts_path).fillna('').set_index('post_id')\n",
    "for col in ['instances', 'ocr', 'verdicts', 'text']:\n",
    "    df_posts[col] = df_posts[col].apply(parse_col)\n",
    "\n",
    "\n",
    "df_fact_check_post_mapping = pd.read_csv(fact_check_post_mapping_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# # Der Pfad zur CSV-Datei\n",
    "# csv_datei = posts_path\n",
    "\n",
    "# # Öffnen der CSV-Datei und auslesen der 'verdicts' Spalte\n",
    "# with open(csv_datei, newline='', encoding='utf-8') as csvfile:\n",
    "#     # CSV-Reader erstellen\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "    \n",
    "#     # Set für einzigartige verdicts-Werte\n",
    "#     verdicts_set = set()\n",
    "\n",
    "#     # Iteriere durch jede Zeile in der CSV-Datei\n",
    "#     for row in reader:\n",
    "#         # Füge den Wert der 'verdicts'-Spalte zum Set hinzu\n",
    "#         verdicts_set.add(row['verdicts'])\n",
    "\n",
    "#     # Ausgabe der einzigartigen 'verdicts'-Werte\n",
    "#     print(\"Mögliche 'verdicts'-Werte:\")\n",
    "#     for verdict in sorted(verdicts_set):\n",
    "#         print(verdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the CSV files\n",
    "# file1 = posts_path\n",
    "# file2 = fact_checks_path\n",
    "# mapping_file = fact_check_post_mapping_path\n",
    "\n",
    "# df1 = pd.read_csv(file1)\n",
    "# df2 = pd.read_csv(file2)\n",
    "# mapping = pd.read_csv(mapping_file)\n",
    "\n",
    "# # Debugging: Check column names\n",
    "# print(\"Columns in df1:\", df1.columns)\n",
    "# print(\"Columns in df2:\", df2.columns)\n",
    "# print(\"Columns in mapping:\", mapping.columns)\n",
    "\n",
    "# # Map IDs in df1 using the mapping file\n",
    "# df1 = df1.merge(mapping, on=\"post_id\", how=\"left\")\n",
    "# df1[\"mapped_id\"] = df1[\"fact_check_id\"]  # Add a new column for mapped IDs\n",
    "# df1 = df1.drop(columns=[\"fact_check_id\"])  # Drop unnecessary columns\n",
    "\n",
    "# # No mapping needed for df2 as it already uses fact_check_id\n",
    "# df2[\"mapped_id\"] = df2[\"fact_check_id\"]  # Rename for consistency\n",
    "\n",
    "# # Merge the two DataFrames using the mapped ID column\n",
    "# merged_df = pd.merge(df1, df2, on=\"mapped_id\", how=\"inner\")\n",
    "\n",
    "# # Save the result to a new CSV file\n",
    "# output_file = \"merged_output.csv\"\n",
    "# merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "# print(f\"Files merged successfully. Output saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file saved to merged_output.csv\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv(\"merged_output.csv\")\n",
    "# output_file = \"merged_output.csv\"\n",
    "# # Specify columns to delete\n",
    "# columns_to_delete = [\"fact_check_id\"]  # Replace with actual column names (\"instances_x\", \"text\", \"mapped_id\", \"instances_y\",)\n",
    "\n",
    "# # Delete the columns\n",
    "# df = df.drop(columns=columns_to_delete)\n",
    "\n",
    "# # Save the updated file\n",
    "# df.to_csv(output_file, index=False)\n",
    "\n",
    "# print(f\"Updated CSV file saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[('! Dreister Impf-Fake von Markus Söder! Es ist wirklich unglaublich, wie dreist Spitzenpolitiker wie beispielsweise Markus Söder uns verarschen. Auf Instagram macht Söder fleißig Werbung für das Impfen Doch wenn man genau hinschaut, sieht man, dass er sich gar nichts injizieren lässt. Der Deckel ist nämlich noch auf der Nadel. Da könnt ihr mal sehen, wie sehr diejenigen, die euch zwangsimpfen wollen, den Impfstoffen vertrauen! markus.soeder FSME ...', \"! Brazen vaccination fake by Markus Söder! It's really unbelievable how bold Top politicians such as Markus Söder kidding us. On Instagram does Söder busy advertising for vaccination But if you look closely, you can see you that he can't be injected at all. The lid is still on the needle. You can see how much those who want to vaccinate you, the Trust vaccines! markus.soeder TBE ...\", [('deu', 0.9577142000198364), ('fy', 0.03401247784495354)])]\n",
      "['False information']\n",
      "('Markus Söder hat seine Impfung vorgetäuscht.', 'Markus Söder faked his vaccination.', [('deu', 1.0)])\n",
      "('Nein, Markus Söder hat seine Impfung nicht vorgetäuscht', 'No, Markus Söder did not fake his vaccination', [('deu', 1.0)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"merged_output.csv\")\n",
    "\n",
    "# Set Pandas options to display full content without truncation\n",
    "pd.set_option(\"display.max_colwidth\", None)  # No truncation for column width\n",
    "pd.set_option(\"display.max_rows\", None)  # Show all rows (use with caution for large datasets)\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "pd.set_option(\"display.width\", None)  # Automatically adjust width\n",
    "\n",
    "# Print the first row of every column\n",
    "for p in df.iloc[0].values:\n",
    "    print(p)  # Access the first row (index 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\schmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\schmi\\AppData\\Local\\Temp\\ipykernel_15252\\3216602316.py:44: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  post_embedding = torch.tensor([self.embedding_matrix[idx] for idx in post_indices], dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (200x300 and 30000x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 101\u001b[0m\n\u001b[0;32m     98\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Forward Pass\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Verlust berechnen\u001b[39;00m\n\u001b[0;32m    104\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[1;32mc:\\Users\\schmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\schmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [3], line 63\u001b[0m, in \u001b[0;36mFactCheckModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 63\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Erste Schicht\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)  \u001b[38;5;66;03m# ReLU Aktivierung\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)  \u001b[38;5;66;03m# Ausgabeschicht\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\schmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\schmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\schmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (200x300 and 30000x512)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Hyperparameter\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "input_size = 300  # Größe des Word Embeddings (z.B. GloVe 300d)\n",
    "hidden_size = 512  # Größe der versteckten Schicht\n",
    "max_length = 50  # Maximale Länge der Eingabesequenz (Nummer der Wörter)\n",
    "\n",
    "# Beispiel für die Word Embedding Initialisierung (z.B. GloVe)\n",
    "embedding_matrix = np.random.rand(10000, input_size)  # 10.000 Vokabulargröße, Embedding-Größe 300\n",
    "\n",
    "class FactCheckDataset(Dataset):\n",
    "    def __init__(self, posts, claims, labels, tokenizer, max_length, embedding_matrix):\n",
    "        self.posts = posts\n",
    "        self.claims = claims\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        post = self.posts[idx]\n",
    "        claim = self.claims[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenisierung und Indexierung der Wörter\n",
    "        post_indices = self.tokenizer(post)\n",
    "        claim_indices = self.tokenizer(claim)\n",
    "        \n",
    "        # Padding und Truncation\n",
    "        post_indices = post_indices[:self.max_length] + [0] * (self.max_length - len(post_indices))\n",
    "        claim_indices = claim_indices[:self.max_length] + [0] * (self.max_length - len(claim_indices))\n",
    "        \n",
    "        # Konvertiere Indizes zu Embeddings\n",
    "        post_embedding = torch.tensor([self.embedding_matrix[idx] for idx in post_indices], dtype=torch.float32)\n",
    "        claim_embedding = torch.tensor([self.embedding_matrix[idx] for idx in claim_indices], dtype=torch.float32)\n",
    "        \n",
    "        # Kombiniere Post und Claim-Embeddings\n",
    "        combined_embedding = torch.cat((post_embedding, claim_embedding), dim=0)  # Verkettung der beiden\n",
    "\n",
    "        return {\n",
    "            'embedding': combined_embedding,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class FactCheckModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FactCheckModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Fully connected layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  # Output layer (2 Klassen: relevant, nicht relevant)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # Erste Schicht\n",
    "        x = self.relu(x)  # ReLU Aktivierung\n",
    "        x = self.fc2(x)  # Ausgabeschicht\n",
    "        return x\n",
    "\n",
    "# Simulierter Tokenizer: wandelt Wörter in Indizes um (in der Realität verwendest du einen echten Tokenizer)\n",
    "def simple_tokenizer(text):\n",
    "    # Dummy-Tokenizer: Splitte Text in Wörter und ordne jedem Wort eine ID zu\n",
    "    word_to_index = {\"post\": 1, \"claim\": 2, \"fact\": 3, \"relevant\": 4, \"irrelevant\": 5}  # Beispielwortschatz\n",
    "    return [word_to_index.get(word, 0) for word in text.lower().split()]\n",
    "\n",
    "# Beispiel-Daten\n",
    "posts = [\"post text 1\", \"post text 2\"]\n",
    "claims = [\"claim text 1\", \"claim text 2\"]\n",
    "labels = [1, 0]  # 1: relevant, 0: nicht relevant\n",
    "\n",
    "# Dataset erstellen\n",
    "dataset = FactCheckDataset(posts, claims, labels, simple_tokenizer, max_length, embedding_matrix)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Modell initialisieren\n",
    "model = FactCheckModel(input_size=2*max_length*input_size, hidden_size=hidden_size, num_classes=2)\n",
    "\n",
    "# Optimierer und Verlustfunktion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        embeddings = batch['embedding']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        # Forward Pass\n",
    "        outputs = model(embeddings)\n",
    "        \n",
    "        # Verlust berechnen\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward Pass und Optimierung\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "# Funktion zur Berechnung der Top 10 relevantesten Behauptungen für einen gegebenen Post\n",
    "def get_top_k_relevant_claims(post, claims, k=10):\n",
    "    # Tokenisiere den Post\n",
    "    post_indices = simple_tokenizer(post)\n",
    "    post_embedding = torch.tensor([embedding_matrix[idx] for idx in post_indices], dtype=torch.float32)\n",
    "    \n",
    "    # Berechne die Ähnlichkeit zwischen dem Post und jeder Behauptung\n",
    "    similarities = []\n",
    "    for claim in claims:\n",
    "        # Tokenisiere die Behauptung\n",
    "        claim_indices = simple_tokenizer(claim)\n",
    "        claim_embedding = torch.tensor([embedding_matrix[idx] for idx in claim_indices], dtype=torch.float32)\n",
    "        \n",
    "        # Berechne die Ähnlichkeit (z.B. Cosine Similarity)\n",
    "        similarity = cosine_similarity(post_embedding.unsqueeze(0), claim_embedding.unsqueeze(0))\n",
    "        similarities.append(similarity[0][0])\n",
    "    \n",
    "    # Sortiere die Behauptungen nach der Ähnlichkeit und gib die Top-k zurück\n",
    "    top_k_indices = np.argsort(similarities)[-k:][::-1]  # Top-k Indizes, absteigend sortiert\n",
    "    return [(claims[idx], similarities[idx]) for idx in top_k_indices]\n",
    "\n",
    "# Beispiel: Top 10 relevante Behauptungen für einen Post\n",
    "post_example = \"post text 1\"\n",
    "top_10_claims = get_top_k_relevant_claims(post_example, claims, k=2)\n",
    "\n",
    "print(\"Top 10 relevante Behauptungen:\")\n",
    "for claim, similarity in top_10_claims:\n",
    "    print(f\"Behauptung: {claim} - Ähnlichkeit: {similarity:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
