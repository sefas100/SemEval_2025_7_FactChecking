{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking path: .\\Data\\posts.csv\n",
      "Checking path: .\\Data\\fact_checks.csv\n",
      "Checking path: .\\Data\\pairs.csv\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "our_dataset_path = '.'\n",
    "\n",
    "posts_path = os.path.join(our_dataset_path, 'Data\\\\posts.csv')\n",
    "fact_checks_path = os.path.join(our_dataset_path, 'Data\\\\fact_checks.csv')\n",
    "fact_check_post_mapping_path = os.path.join(our_dataset_path, 'Data\\\\pairs.csv')\n",
    "\n",
    "for path in [posts_path, fact_checks_path, fact_check_post_mapping_path]:\n",
    "    print(f\"Checking path: {path}\")\n",
    "    assert os.path.isfile(path), f\"File not found: {path}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_col = lambda s: ast.literal_eval(s.replace('\\n', '\\\\n')) if s else s\n",
    "\n",
    "df_fact_checks = pd.read_csv(fact_checks_path).fillna('').set_index('fact_check_id')\n",
    "for col in ['claim', 'instances', 'title']:\n",
    "    df_fact_checks[col] = df_fact_checks[col].apply(parse_col)\n",
    "\n",
    "\n",
    "df_posts = pd.read_csv(posts_path).fillna('').set_index('post_id')\n",
    "for col in ['instances', 'ocr', 'verdicts', 'text']:\n",
    "    df_posts[col] = df_posts[col].apply(parse_col)\n",
    "\n",
    "\n",
    "df_fact_check_post_mapping = pd.read_csv(fact_check_post_mapping_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# # Der Pfad zur CSV-Datei\n",
    "# csv_datei = posts_path\n",
    "\n",
    "# # Öffnen der CSV-Datei und auslesen der 'verdicts' Spalte\n",
    "# with open(csv_datei, newline='', encoding='utf-8') as csvfile:\n",
    "#     # CSV-Reader erstellen\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "    \n",
    "#     # Set für einzigartige verdicts-Werte\n",
    "#     verdicts_set = set()\n",
    "\n",
    "#     # Iteriere durch jede Zeile in der CSV-Datei\n",
    "#     for row in reader:\n",
    "#         # Füge den Wert der 'verdicts'-Spalte zum Set hinzu\n",
    "#         verdicts_set.add(row['verdicts'])\n",
    "\n",
    "#     # Ausgabe der einzigartigen 'verdicts'-Werte\n",
    "#     print(\"Mögliche 'verdicts'-Werte:\")\n",
    "#     for verdict in sorted(verdicts_set):\n",
    "#         print(verdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the CSV files\n",
    "# file1 = posts_path\n",
    "# file2 = fact_checks_path\n",
    "# mapping_file = fact_check_post_mapping_path\n",
    "\n",
    "# df1 = pd.read_csv(file1)\n",
    "# df2 = pd.read_csv(file2)\n",
    "# mapping = pd.read_csv(mapping_file)\n",
    "\n",
    "# # Debugging: Check column names\n",
    "# print(\"Columns in df1:\", df1.columns)\n",
    "# print(\"Columns in df2:\", df2.columns)\n",
    "# print(\"Columns in mapping:\", mapping.columns)\n",
    "\n",
    "# # Map IDs in df1 using the mapping file\n",
    "# df1 = df1.merge(mapping, on=\"post_id\", how=\"left\")\n",
    "# df1[\"mapped_id\"] = df1[\"fact_check_id\"]  # Add a new column for mapped IDs\n",
    "# df1 = df1.drop(columns=[\"fact_check_id\"])  # Drop unnecessary columns\n",
    "\n",
    "# # No mapping needed for df2 as it already uses fact_check_id\n",
    "# df2[\"mapped_id\"] = df2[\"fact_check_id\"]  # Rename for consistency\n",
    "\n",
    "# # Merge the two DataFrames using the mapped ID column\n",
    "# merged_df = pd.merge(df1, df2, on=\"mapped_id\", how=\"inner\")\n",
    "\n",
    "# # Save the result to a new CSV file\n",
    "# output_file = \"merged_output.csv\"\n",
    "# merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "# print(f\"Files merged successfully. Output saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file saved to merged_output.csv\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv(\"merged_output.csv\")\n",
    "# output_file = \"merged_output.csv\"\n",
    "# # Specify columns to delete\n",
    "# columns_to_delete = [\"fact_check_id\"]  # Replace with actual column names (\"instances_x\", \"text\", \"mapped_id\", \"instances_y\",)\n",
    "\n",
    "# # Delete the columns\n",
    "# df = df.drop(columns=columns_to_delete)\n",
    "\n",
    "# # Save the updated file\n",
    "# df.to_csv(output_file, index=False)\n",
    "\n",
    "# print(f\"Updated CSV file saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[('! Dreister Impf-Fake von Markus Söder! Es ist wirklich unglaublich, wie dreist Spitzenpolitiker wie beispielsweise Markus Söder uns verarschen. Auf Instagram macht Söder fleißig Werbung für das Impfen Doch wenn man genau hinschaut, sieht man, dass er sich gar nichts injizieren lässt. Der Deckel ist nämlich noch auf der Nadel. Da könnt ihr mal sehen, wie sehr diejenigen, die euch zwangsimpfen wollen, den Impfstoffen vertrauen! markus.soeder FSME ...', \"! Brazen vaccination fake by Markus Söder! It's really unbelievable how bold Top politicians such as Markus Söder kidding us. On Instagram does Söder busy advertising for vaccination But if you look closely, you can see you that he can't be injected at all. The lid is still on the needle. You can see how much those who want to vaccinate you, the Trust vaccines! markus.soeder TBE ...\", [('deu', 0.9577142000198364), ('fy', 0.03401247784495354)])]\n",
      "['False information']\n",
      "('Markus Söder hat seine Impfung vorgetäuscht.', 'Markus Söder faked his vaccination.', [('deu', 1.0)])\n",
      "('Nein, Markus Söder hat seine Impfung nicht vorgetäuscht', 'No, Markus Söder did not fake his vaccination', [('deu', 1.0)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"merged_output.csv\")\n",
    "\n",
    "# Set Pandas options to display full content without truncation\n",
    "pd.set_option(\"display.max_colwidth\", None)  # No truncation for column width\n",
    "pd.set_option(\"display.max_rows\", None)  # Show all rows (use with caution for large datasets)\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "pd.set_option(\"display.width\", None)  # Automatically adjust width\n",
    "\n",
    "# Print the first row of every column\n",
    "for p in df.iloc[0].values:\n",
    "    print(p)  # Access the first row (index 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Hyperparameter\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "input_size = 300  # Größe des Word Embeddings (z.B. GloVe 300d)\n",
    "hidden_size = 512  # Größe der versteckten Schicht\n",
    "max_length = 50  # Maximale Länge der Eingabesequenz (Nummer der Wörter)\n",
    "\n",
    "# Beispiel für die Word Embedding Initialisierung (z.B. GloVe)\n",
    "embedding_matrix = np.random.rand(10000, input_size)  # 10.000 Vokabulargröße, Embedding-Größe 300\n",
    "\n",
    "class FactCheckDataset(Dataset):\n",
    "    def __init__(self, posts, claims, labels, tokenizer, max_length, embedding_matrix):\n",
    "        self.posts = posts\n",
    "        self.claims = claims\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        post = self.posts[idx]\n",
    "        claim = self.claims[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenisierung und Indexierung der Wörter\n",
    "        post_indices = self.tokenizer(post)\n",
    "        claim_indices = self.tokenizer(claim)\n",
    "        \n",
    "        # Padding und Truncation\n",
    "        post_indices = post_indices[:self.max_length] + [0] * (self.max_length - len(post_indices))\n",
    "        claim_indices = claim_indices[:self.max_length] + [0] * (self.max_length - len(claim_indices))\n",
    "        \n",
    "        # Konvertiere Indizes zu Embeddings\n",
    "        post_embedding = torch.tensor([self.embedding_matrix[idx] for idx in post_indices], dtype=torch.float32)\n",
    "        claim_embedding = torch.tensor([self.embedding_matrix[idx] for idx in claim_indices], dtype=torch.float32)\n",
    "        \n",
    "        # Kombiniere Post und Claim-Embeddings\n",
    "        combined_embedding = torch.cat((post_embedding, claim_embedding), dim=0)  # Verkettung der beiden\n",
    "\n",
    "        return {\n",
    "            'embedding': combined_embedding,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class FactCheckModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FactCheckModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Fully connected layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  # Output layer (2 Klassen: relevant, nicht relevant)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # Erste Schicht\n",
    "        x = self.relu(x)  # ReLU Aktivierung\n",
    "        x = self.fc2(x)  # Ausgabeschicht\n",
    "        return x\n",
    "\n",
    "# Simulierter Tokenizer: wandelt Wörter in Indizes um (in der Realität verwendest du einen echten Tokenizer)\n",
    "def simple_tokenizer(text):\n",
    "    # Dummy-Tokenizer: Splitte Text in Wörter und ordne jedem Wort eine ID zu\n",
    "    word_to_index = {\"post\": 1, \"claim\": 2, \"fact\": 3, \"relevant\": 4, \"irrelevant\": 5}  # Beispielwortschatz\n",
    "    return [word_to_index.get(word, 0) for word in text.lower().split()]\n",
    "\n",
    "# Beispiel-Daten\n",
    "posts = [\"post text 1\", \"post text 2\"]\n",
    "claims = [\"claim text 1\", \"claim text 2\"]\n",
    "labels = [1, 0]  # 1: relevant, 0: nicht relevant\n",
    "\n",
    "# Dataset erstellen\n",
    "dataset = FactCheckDataset(posts, claims, labels, simple_tokenizer, max_length, embedding_matrix)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Modell initialisieren\n",
    "model = FactCheckModel(input_size=2*max_length*input_size, hidden_size=hidden_size, num_classes=2)\n",
    "\n",
    "# Optimierer und Verlustfunktion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        embeddings = batch['embedding']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        # Forward Pass\n",
    "        outputs = model(embeddings)\n",
    "        \n",
    "        # Verlust berechnen\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward Pass und Optimierung\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "# Funktion zur Berechnung der Top 10 relevantesten Behauptungen für einen gegebenen Post\n",
    "def get_top_k_relevant_claims(post, claims, k=10):\n",
    "    # Tokenisiere den Post\n",
    "    post_indices = simple_tokenizer(post)\n",
    "    post_embedding = torch.tensor([embedding_matrix[idx] for idx in post_indices], dtype=torch.float32)\n",
    "    \n",
    "    # Berechne die Ähnlichkeit zwischen dem Post und jeder Behauptung\n",
    "    similarities = []\n",
    "    for claim in claims:\n",
    "        # Tokenisiere die Behauptung\n",
    "        claim_indices = simple_tokenizer(claim)\n",
    "        claim_embedding = torch.tensor([embedding_matrix[idx] for idx in claim_indices], dtype=torch.float32)\n",
    "        \n",
    "        # Berechne die Ähnlichkeit (z.B. Cosine Similarity)\n",
    "        similarity = cosine_similarity(post_embedding.unsqueeze(0), claim_embedding.unsqueeze(0))\n",
    "        similarities.append(similarity[0][0])\n",
    "    \n",
    "    # Sortiere die Behauptungen nach der Ähnlichkeit und gib die Top-k zurück\n",
    "    top_k_indices = np.argsort(similarities)[-k:][::-1]  # Top-k Indizes, absteigend sortiert\n",
    "    return [(claims[idx], similarities[idx]) for idx in top_k_indices]\n",
    "\n",
    "# Beispiel: Top 10 relevante Behauptungen für einen Post\n",
    "post_example = \"post text 1\"\n",
    "top_10_claims = get_top_k_relevant_claims(post_example, claims, k=2)\n",
    "\n",
    "print(\"Top 10 relevante Behauptungen:\")\n",
    "for claim, similarity in top_10_claims:\n",
    "    print(f\"Behauptung: {claim} - Ähnlichkeit: {similarity:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
