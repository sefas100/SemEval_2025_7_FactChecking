{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking path: .\\Data\\posts.csv\n",
      "Checking path: .\\Data\\fact_checks.csv\n",
      "Checking path: .\\Data\\pairs.csv\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "our_dataset_path = '.'\n",
    "\n",
    "posts_path = os.path.join(our_dataset_path, 'Data\\\\posts.csv')\n",
    "fact_checks_path = os.path.join(our_dataset_path, 'Data\\\\fact_checks.csv')\n",
    "fact_check_post_mapping_path = os.path.join(our_dataset_path, 'Data\\\\pairs.csv')\n",
    "\n",
    "for path in [posts_path, fact_checks_path, fact_check_post_mapping_path]:\n",
    "    print(f\"Checking path: {path}\")\n",
    "    assert os.path.isfile(path), f\"File not found: {path}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_col = lambda s: ast.literal_eval(s.replace('\\n', '\\\\n')) if s else s\n",
    "\n",
    "df_fact_checks = pd.read_csv(fact_checks_path).fillna('').set_index('fact_check_id')\n",
    "for col in ['claim', 'instances', 'title']:\n",
    "    df_fact_checks[col] = df_fact_checks[col].apply(parse_col)\n",
    "\n",
    "\n",
    "df_posts = pd.read_csv(posts_path).fillna('').set_index('post_id')\n",
    "for col in ['instances', 'ocr', 'verdicts', 'text']:\n",
    "    df_posts[col] = df_posts[col].apply(parse_col)\n",
    "\n",
    "\n",
    "df_fact_check_post_mapping = pd.read_csv(fact_check_post_mapping_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mögliche 'verdicts'-Werte:\n",
      "[' Sensitive content ']\n",
      "['Altered Photo/Video']\n",
      "['Altered photo']\n",
      "['Altered photo/video']\n",
      "['Altered photo/video.']\n",
      "['Altered video']\n",
      "['False headline']\n",
      "['False information and graphic content']\n",
      "['False information', 'False information and graphic content']\n",
      "['False information', 'False information.']\n",
      "['False information', 'Missing Context']\n",
      "['False information', 'Missing context']\n",
      "['False information', 'Partly False']\n",
      "['False information', 'Partly false information']\n",
      "['False information', 'Partly false information.']\n",
      "['False information']\n",
      "['False information.', 'Partly false information']\n",
      "['False information.']\n",
      "['False', 'False information']\n",
      "['False', 'Partly false information']\n",
      "['False']\n",
      "['Missing Context', 'Missing context']\n",
      "['Missing Context']\n",
      "['Missing context', 'Missing context.']\n",
      "['Missing context', 'Partly false information']\n",
      "['Missing context']\n",
      "['Missing context.']\n",
      "['Partly False', 'Partly false information']\n",
      "['Partly False']\n",
      "['Partly false information', 'Partly false information.']\n",
      "['Partly false information']\n",
      "['Partly false information.']\n",
      "['Support your favourite streamers by sending them stars.']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Der Pfad zur CSV-Datei\n",
    "csv_datei = posts_path\n",
    "\n",
    "# Öffnen der CSV-Datei und auslesen der 'verdicts' Spalte\n",
    "with open(csv_datei, newline='', encoding='utf-8') as csvfile:\n",
    "    # CSV-Reader erstellen\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    \n",
    "    # Set für einzigartige verdicts-Werte\n",
    "    verdicts_set = set()\n",
    "\n",
    "    # Iteriere durch jede Zeile in der CSV-Datei\n",
    "    for row in reader:\n",
    "        # Füge den Wert der 'verdicts'-Spalte zum Set hinzu\n",
    "        verdicts_set.add(row['verdicts'])\n",
    "\n",
    "    # Ausgabe der einzigartigen 'verdicts'-Werte\n",
    "    print(\"Mögliche 'verdicts'-Werte:\")\n",
    "    for verdict in sorted(verdicts_set):\n",
    "        print(verdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Hyperparameter\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "input_size = 300  # Größe des Word Embeddings (z.B. GloVe 300d)\n",
    "hidden_size = 512  # Größe der versteckten Schicht\n",
    "max_length = 50  # Maximale Länge der Eingabesequenz (Nummer der Wörter)\n",
    "\n",
    "# Beispiel für die Word Embedding Initialisierung (z.B. GloVe)\n",
    "embedding_matrix = np.random.rand(10000, input_size)  # 10.000 Vokabulargröße, Embedding-Größe 300\n",
    "\n",
    "class FactCheckDataset(Dataset):\n",
    "    def __init__(self, posts, claims, labels, tokenizer, max_length, embedding_matrix):\n",
    "        self.posts = posts\n",
    "        self.claims = claims\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        post = self.posts[idx]\n",
    "        claim = self.claims[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenisierung und Indexierung der Wörter\n",
    "        post_indices = self.tokenizer(post)\n",
    "        claim_indices = self.tokenizer(claim)\n",
    "        \n",
    "        # Padding und Truncation\n",
    "        post_indices = post_indices[:self.max_length] + [0] * (self.max_length - len(post_indices))\n",
    "        claim_indices = claim_indices[:self.max_length] + [0] * (self.max_length - len(claim_indices))\n",
    "        \n",
    "        # Konvertiere Indizes zu Embeddings\n",
    "        post_embedding = torch.tensor([self.embedding_matrix[idx] for idx in post_indices], dtype=torch.float32)\n",
    "        claim_embedding = torch.tensor([self.embedding_matrix[idx] for idx in claim_indices], dtype=torch.float32)\n",
    "        \n",
    "        # Kombiniere Post und Claim-Embeddings\n",
    "        combined_embedding = torch.cat((post_embedding, claim_embedding), dim=0)  # Verkettung der beiden\n",
    "\n",
    "        return {\n",
    "            'embedding': combined_embedding,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class FactCheckModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FactCheckModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Fully connected layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  # Output layer (2 Klassen: relevant, nicht relevant)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # Erste Schicht\n",
    "        x = self.relu(x)  # ReLU Aktivierung\n",
    "        x = self.fc2(x)  # Ausgabeschicht\n",
    "        return x\n",
    "\n",
    "# Simulierter Tokenizer: wandelt Wörter in Indizes um (in der Realität verwendest du einen echten Tokenizer)\n",
    "def simple_tokenizer(text):\n",
    "    # Dummy-Tokenizer: Splitte Text in Wörter und ordne jedem Wort eine ID zu\n",
    "    word_to_index = {\"post\": 1, \"claim\": 2, \"fact\": 3, \"relevant\": 4, \"irrelevant\": 5}  # Beispielwortschatz\n",
    "    return [word_to_index.get(word, 0) for word in text.lower().split()]\n",
    "\n",
    "# Beispiel-Daten\n",
    "posts = [\"post text 1\", \"post text 2\"]\n",
    "claims = [\"claim text 1\", \"claim text 2\"]\n",
    "labels = [1, 0]  # 1: relevant, 0: nicht relevant\n",
    "\n",
    "# Dataset erstellen\n",
    "dataset = FactCheckDataset(posts, claims, labels, simple_tokenizer, max_length, embedding_matrix)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Modell initialisieren\n",
    "model = FactCheckModel(input_size=2*max_length*input_size, hidden_size=hidden_size, num_classes=2)\n",
    "\n",
    "# Optimierer und Verlustfunktion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        embeddings = batch['embedding']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        # Forward Pass\n",
    "        outputs = model(embeddings)\n",
    "        \n",
    "        # Verlust berechnen\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward Pass und Optimierung\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "# Funktion zur Berechnung der Top 10 relevantesten Behauptungen für einen gegebenen Post\n",
    "def get_top_k_relevant_claims(post, claims, k=10):\n",
    "    # Tokenisiere den Post\n",
    "    post_indices = simple_tokenizer(post)\n",
    "    post_embedding = torch.tensor([embedding_matrix[idx] for idx in post_indices], dtype=torch.float32)\n",
    "    \n",
    "    # Berechne die Ähnlichkeit zwischen dem Post und jeder Behauptung\n",
    "    similarities = []\n",
    "    for claim in claims:\n",
    "        # Tokenisiere die Behauptung\n",
    "        claim_indices = simple_tokenizer(claim)\n",
    "        claim_embedding = torch.tensor([embedding_matrix[idx] for idx in claim_indices], dtype=torch.float32)\n",
    "        \n",
    "        # Berechne die Ähnlichkeit (z.B. Cosine Similarity)\n",
    "        similarity = cosine_similarity(post_embedding.unsqueeze(0), claim_embedding.unsqueeze(0))\n",
    "        similarities.append(similarity[0][0])\n",
    "    \n",
    "    # Sortiere die Behauptungen nach der Ähnlichkeit und gib die Top-k zurück\n",
    "    top_k_indices = np.argsort(similarities)[-k:][::-1]  # Top-k Indizes, absteigend sortiert\n",
    "    return [(claims[idx], similarities[idx]) for idx in top_k_indices]\n",
    "\n",
    "# Beispiel: Top 10 relevante Behauptungen für einen Post\n",
    "post_example = \"post text 1\"\n",
    "top_10_claims = get_top_k_relevant_claims(post_example, claims, k=2)\n",
    "\n",
    "print(\"Top 10 relevante Behauptungen:\")\n",
    "for claim, similarity in top_10_claims:\n",
    "    print(f\"Behauptung: {claim} - Ähnlichkeit: {similarity:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
